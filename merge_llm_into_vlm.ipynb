{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to create a custom vision-language model by combining the vision encoder weights with the language model's weights, adjusting configurations as needed to ensure compatibility. Specifically, we will demonstrates how to integrate a pre-trained vision encoder (from Qwen2.5-VL-7B-Instruct) into a smaller language model (Qwen2.5-0.5B-Instruct)\n",
    "\n",
    "Since Qwen already has a vision version of it, we can directly modify the config and adjust the LLM related part so that we can use it with the Qwen 2.5 0.5B Instruct model.\n",
    "Let's see the difference between the config.json of the language model (LM) and the vision language model (VLM)\n",
    "<br>\n",
    "<img src=\"/home/jackson/vision-r1/static/diff_1.png\" width=\"400\"> <br>\n",
    "<img src=\"/home/jackson/vision-r1/static/diff_2.png\" width=\"200\">\n",
    "\n",
    "A few difference that we can notice is:\n",
    "1. Their architecture different (Qwen2ForCausalLM and Qwen2_5_VLForConditionalGeneration), the later is architecture that incorporate vision.\n",
    "2. VLM has additional tokens for vision (We will need to inspect if those token are absent in LM, we will need to add those token)\n",
    "3. Difference in hidden_states, num_hidden_layers, num_attention_heads..etc. \n",
    "4. VLM has extra key for `vision_config`\n",
    "5. The `out_hidden_size` of vision model is 2048 while the `hidden_states` of LM is 896, we will need to modify the last layer of vision model to make them compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackson/vision-r1/vr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    ")\n",
    "from collections import OrderedDict\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "lm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking additional tokens\n",
    "If we check the Qwen VL model, we can observe the `vocab_size` is 151936, which is the same as the LM. In their config, they have these special tokens which indicates the vision related tokens. Let's check if the LM contains the same token in its vocabulary.\n",
    "\"`vision_start_token_id`\": 151652, <br>\n",
    "\"`vision_end_token_id`\": 151653,<br>\n",
    "\"`vision_token_id`\": 151654,<br>\n",
    "\"`image_token_id`\": 151655,<br>\n",
    "\"`video_token_id`\": 151656,<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|vision_end|>\n",
      "<|vision_pad|>\n",
      "<|image_pad|>\n",
      "<|video_pad|>\n"
     ]
    }
   ],
   "source": [
    "vision_tokens = [151653, 151654, 151655, 151656]\n",
    "for vt in vision_tokens:\n",
    "    print(tokenizer._convert_id_to_token(vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the LM already has those token! So we don't need to add any new tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine vision encoder with base model\n",
    "One simplest approach is to create a new config.json file that specifically fits the vision encoder and Qwen 0.5, use it to create a new model architecture, then load the corresponding pretrained weights into it.\n",
    "\n",
    "We can simply use the vision model config.json and modify from there.\n",
    "\n",
    "This is the new config file that match the LM config (such as `hidden_states`, `intermediate_size`..etc)\n",
    "```json\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"Qwen2_5_VLForConditionalGeneration\"\n",
    "  ],\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 151643,\n",
    "  \"eos_token_id\": 151645,\n",
    "  \"vision_start_token_id\": 151652,\n",
    "  \"vision_end_token_id\": 151653,\n",
    "  \"vision_token_id\": 151654,\n",
    "  \"image_token_id\": 151655,\n",
    "  \"video_token_id\": 151656,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 896,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 4864,\n",
    "  \"max_position_embeddings\": 32768,\n",
    "  \"max_window_layers\": 21,\n",
    "  \"model_type\": \"qwen2_5_vl\",\n",
    "  \"num_attention_heads\": 14,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"num_key_value_heads\": 2,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"rope_theta\": 1000000.0,\n",
    "  \"sliding_window\": 32768,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.43.1\",\n",
    "  \"use_cache\": true,\n",
    "  \"use_sliding_window\": false,\n",
    "  \"vision_config\": {\n",
    "    \"depth\": 32,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 1280,\n",
    "    \"intermediate_size\": 3420,\n",
    "    \"num_heads\": 16,\n",
    "    \"in_chans\": 3,\n",
    "    \"out_hidden_size\": 896,\n",
    "    \"patch_size\": 14,\n",
    "    \"spatial_merge_size\": 2,\n",
    "    \"spatial_patch_size\": 14,\n",
    "    \"window_size\": 112,\n",
    "    \"fullatt_block_indexes\": [\n",
    "      7,\n",
    "      15,\n",
    "      23,\n",
    "      31\n",
    "    ],\n",
    "    \"tokens_per_second\": 2,\n",
    "    \"temporal_patch_size\": 2\n",
    "  },\n",
    "  \"rope_scaling\": {\n",
    "    \"type\": \"mrope\",\n",
    "    \"mrope_section\": [\n",
    "      16,\n",
    "      24,\n",
    "      24\n",
    "    ]\n",
    "  },\n",
    "  \"vocab_size\": 151936\n",
    "}\n",
    "```\n",
    "We can now load the custom model with our new config.json file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"qwen-0.5-vl-config.json\")\n",
    "custom_model = Qwen2_5_VLForConditionalGeneration._from_config(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step is to save the weights of `vision_encoder` from VLM, and LM weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# save vision encoder weights\n",
    "vlm = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"/tmp2/jackson/Qwen--Qwen2.5-VL-7B-Instruct\",\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "visual_weights_path = \"visual_weights.pth\"\n",
    "torch.save(vlm.visual.state_dict(), visual_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lm weights\n",
    "lm_weights_path = \"lm_weights.pth\"\n",
    "torch.save(lm.state_dict(), lm_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the saved weights and load it into our custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved vision encoder and LM weights\n",
    "visual_state_dict = torch.load(\"visual_weights.pth\", weights_only=True)\n",
    "llm_state_dict = torch.load(\"lm_weights.pth\", weights_only=True)\n",
    "modified_visual_state_dict = OrderedDict(\n",
    "    (f\"visual.{key}\", value) for key, value in visual_state_dict.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output shape of last layer is vision encoder does not match with the LM `hidden_states`. Thus, we will need to modify its last layer. Instead of randomly initialize the weights or add a new projection layer, we slice the original weights for simplicity. (eg, `output_hidden_size` for Qwen 2.5 VL 7B is  3584, but we will take the first 896 (`hidden_states` for LM)) as the new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3584, 5120])\n",
      "torch.Size([3584])\n",
      "Modified\n",
      "torch.Size([896, 5120])\n",
      "torch.Size([896])\n"
     ]
    }
   ],
   "source": [
    "print(modified_visual_state_dict[\"visual.merger.mlp.2.weight\"].shape)\n",
    "print(modified_visual_state_dict[\"visual.merger.mlp.2.bias\"].shape)\n",
    "new_output_shape = 896\n",
    "modified_visual_state_dict[\"visual.merger.mlp.2.weight\"] = modified_visual_state_dict[\n",
    "    \"visual.merger.mlp.2.weight\"\n",
    "][:new_output_shape, :]\n",
    "modified_visual_state_dict[\"visual.merger.mlp.2.bias\"] = modified_visual_state_dict[\n",
    "    \"visual.merger.mlp.2.bias\"\n",
    "][:new_output_shape]\n",
    "print(\"Modified\")\n",
    "print(modified_visual_state_dict[\"visual.merger.mlp.2.weight\"].shape)\n",
    "print(modified_visual_state_dict[\"visual.merger.mlp.2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_1 = custom_model.load_state_dict(modified_visual_state_dict, strict=False)\n",
    "miss_2 = custom_model.load_state_dict(llm_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_miss = miss_1.missing_keys + miss_2.missing_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if any missing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss_1 should contain missing keys from llm if there is any\n",
    "# miss_2 should contain missing keys from visual if there is any\n",
    "for name, param in custom_model.named_parameters():\n",
    "    if name in total_miss:\n",
    "        continue\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.save_pretrained(\"qwen-0.5-vl-custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Now we have successfully integrate the well-trained `vision_encoder` and LM weights! We can proceed to fine-tune them! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
